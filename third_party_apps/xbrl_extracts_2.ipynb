{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<div style=\"\n",
    "    width: 100%;\n",
    "    height: 100px;\n",
    "    border:none;\n",
    "    background:linear-gradient(-45deg, #d84227, #ce3587);\n",
    "    text-align: center;\n",
    "    position: relative;\n",
    "\">\n",
    "  <span style=\"\n",
    "      font-size: 40px;\n",
    "      padding: 0 10px;\n",
    "      color: white;\n",
    "      margin: 0;\n",
    "      position: absolute;\n",
    "      top: 50%;\n",
    "      left: 50%;\n",
    "      -ms-transform: translate(-50%, -50%);\n",
    "      transform: translate(-50%, -50%);\n",
    "      font-weight: bold;\n",
    "  \">\n",
    "    XBRL PARSER <!--Padding is optional-->\n",
    "  </span>\n",
    "</div><a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Martin Wood - Office for National Statistics\n",
    "martin.wood@ons.gov.uk\n",
    "23/07/2018\n",
    "XBRL parser\n",
    "Contains functions that scrape and clean an XBRL document's content\n",
    "and variables, returning a dict ready for dumping into\n",
    "MongoDB.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from bs4 import BeautifulSoup as BS  # Can parse xml or html docs\n",
    "\n",
    "# Table of variables and values that indicate consolidated status\n",
    "consolidation_var_table = {\n",
    "    \"includedinconsolidationsubsidiary\": True,\n",
    "    \"investmententityrequiredtoapplyexceptionfromconsolidationtruefalse\": True,\n",
    "    \"subsidiaryunconsolidatedtruefalse\": False,\n",
    "    \"descriptionreasonwhyentityhasnotpreparedconsolidatedfinancialstatements\": \"exist\",\n",
    "    \"consolidationpolicy\": \"exist\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_value(string):\n",
    "    \"\"\"\n",
    "    Take a value that's stored as a string,\n",
    "    clean it and convert to numeric.\n",
    "\n",
    "    If it's just a dash, it's taken to mean\n",
    "    zero.\n",
    "    \"\"\"\n",
    "    if string.strip() == \"-\":\n",
    "        return (0.0)\n",
    "\n",
    "    try:\n",
    "        return float(string.strip().replace(\",\", \"\").replace(\" \", \"\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (string)\n",
    "\n",
    "\n",
    "def retrieve_from_context(soup, contextref):\n",
    "    \"\"\"\n",
    "    Used where an element of the document contained no data, only a\n",
    "    reference to a context element.\n",
    "    Finds the relevant context element and retrieves the relevant data.\n",
    "\n",
    "    Returns a text string\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    contextref -- the id of the context element to be raided\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        context = soup.find(\"xbrli:context\", id=contextref)\n",
    "        contents = context.find(\"xbrldi:explicitmember\").get_text().split(\":\")[-1].strip()\n",
    "\n",
    "    except:\n",
    "        contents = \"\"\n",
    "\n",
    "    return (contents)\n",
    "\n",
    "\n",
    "def retrieve_accounting_standard(soup):\n",
    "    \"\"\"\n",
    "    Gets the account reporting standard in use in a document by hunting\n",
    "    down the link to the schema reference sheet that always appears to\n",
    "    be in the document, and extracting the format and standard date from\n",
    "    the string of the url itself.\n",
    "    WARNING - That means that there's a lot of implicity hardcoded info\n",
    "    on the way these links are formated and referenced, within this\n",
    "    function.  Might need changing someday.\n",
    "\n",
    "    Returns a 3-tuple (standard, date, original url)\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the relevant link by its unique attribute\n",
    "    link_obj = soup.find(\"link:schemaref\")\n",
    "\n",
    "    # If we didn't find anything it's an xml doc using a different\n",
    "    # element name:\n",
    "    if link_obj == None:\n",
    "        link_obj = soup.find(\"schemaref\")\n",
    "\n",
    "    # extract the name of the .xsd schema file, which contains format\n",
    "    # and date information\n",
    "    text = link_obj['xlink:href'].split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Split the extracted text into format and date, return values\n",
    "    return (text[:-10].strip(\"-\"), text[-10:], link_obj['xlink:href'])\n",
    "\n",
    "\n",
    "def retrieve_unit(soup, each):\n",
    "    \"\"\"\n",
    "    Gets the reporting unit by trying to chase a unitref to\n",
    "    its source, alternatively uses element attribute unitref\n",
    "    if it's not a reference to another element.\n",
    "\n",
    "    Returns the unit\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    each -- element of BeautifulSoup souped object\n",
    "    \"\"\"\n",
    "\n",
    "    # If not, try to discover the unit string in the\n",
    "    # soup object\n",
    "    try:\n",
    "        unit_str = soup.find(id=each['unitref']).get_text()\n",
    "\n",
    "    except:\n",
    "        # Or if not, in the attributes of the element\n",
    "        try:\n",
    "            unit_str = each.attrs['unitref']\n",
    "\n",
    "        except:\n",
    "            return (\"NA\")\n",
    "\n",
    "    return (unit_str.strip())\n",
    "\n",
    "\n",
    "def retrieve_date(soup, each):\n",
    "    \"\"\"\n",
    "    Gets the reporting date by trying to chase a contextref\n",
    "    to its source and extract its period, alternatively uses\n",
    "    element attribute contextref if it's not a reference\n",
    "    to another element.\n",
    "    Returns the date\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    each -- element of BeautifulSoup souped object\n",
    "    \"\"\"\n",
    "\n",
    "    # Try to find a date tag within the contextref element,\n",
    "    # starting with the most specific tags, and starting with\n",
    "    # those for ixbrl docs as it's the most common file.\n",
    "    date_tag_list = [\"xbrli:enddate\",\n",
    "                     \"xbrli:instant\",\n",
    "                     \"xbrli:period\",\n",
    "                     \"enddate\",\n",
    "                     \"instant\",\n",
    "                     \"period\"]\n",
    "\n",
    "    for tag in date_tag_list:\n",
    "        try:\n",
    "            date_str = each['contextref']\n",
    "            date_val = parser.parse(soup.find(id=each['contextref']).find(tag).get_text()). \\\n",
    "                date(). \\\n",
    "                isoformat()\n",
    "            return (date_val)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        date_str = each.attrs['contextref']\n",
    "        date_val = parser.parse(each.attrs['contextref']). \\\n",
    "            date(). \\\n",
    "            isoformat()\n",
    "        return (date_val)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (\"NA\")\n",
    "\n",
    "\n",
    "def parse_element(soup, element):\n",
    "    \"\"\"\n",
    "    For a discovered XBRL tagged element, go through, retrieve its name\n",
    "    and value and associated metadata.\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup object of accounts document\n",
    "    element -- soup object of discovered tagged element\n",
    "    \"\"\"\n",
    "\n",
    "    if \"contextref\" not in element.attrs:\n",
    "        return ({})\n",
    "\n",
    "    element_dict = {}\n",
    "\n",
    "    # Basic name and value\n",
    "    try:\n",
    "        # Method for XBRLi docs first\n",
    "        element_dict['name'] = element.attrs['name'].lower().split(\":\")[-1]\n",
    "    except:\n",
    "        # Method for XBRL docs second\n",
    "        element_dict['name'] = element.name.lower().split(\":\")[-1]\n",
    "\n",
    "    element_dict['value'] = element.get_text()\n",
    "    element_dict['unit'] = retrieve_unit(soup, element)\n",
    "    element_dict['date'] = retrieve_date(soup, element)\n",
    "\n",
    "    # If there's no value retrieved, try raiding the associated context data\n",
    "    if element_dict['value'] == \"\":\n",
    "        element_dict['value'] = retrieve_from_context(soup, element.attrs['contextref'])\n",
    "\n",
    "    # If the value has a defined unit (eg a currency) convert to numeric\t\n",
    "    if element_dict['unit'] != \"NA\":\n",
    "        element_dict['value'] = clean_value(element_dict['value'])\n",
    "\n",
    "    # Retrieve sign of element if exists\n",
    "    try:\n",
    "        element_dict['sign'] = element.attrs['sign']\n",
    "\n",
    "        # if it's negative, convert the value then and there\n",
    "        if element_dict['sign'].strip() == \"-\":\n",
    "            element_dict['value'] = 0.0 - element_dict['value']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (element_dict)\n",
    "\n",
    "\n",
    "def parse_elements(element_set, soup):\n",
    "    \"\"\"\n",
    "    For a set of discovered elements within a document, try to parse\n",
    "    them.  Only keep valid results (test is whether field \"name\"\n",
    "    exists).\n",
    "\n",
    "    Keyword arguments:\n",
    "    element_set -- BeautifulSoup iterable search result object\n",
    "    soup -- BeautifulSoup object of accounts document\n",
    "    \"\"\"\n",
    "    elements = []\n",
    "    for each in element_set:\n",
    "        element_dict = parse_element(soup, each)\n",
    "        if 'name' in element_dict:\n",
    "            elements.append(element_dict)\n",
    "    return (elements)\n",
    "\n",
    "\n",
    "def summarise_by_sum(doc, variable_names):\n",
    "    \"\"\"\n",
    "    Takes a document (dict) after extraction, and tries to extract\n",
    "    a summary variable relating to the financial state of the enterprise\n",
    "    by summing all those named that exist.  Returns dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    doc -- an extracted document dict, with \"elements\" entry as created\n",
    "           by the 'scrape_clean_elements' functions.\n",
    "    variable_names - variables to find and sum if they exist\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert elements to pandas df\n",
    "    df = pd.DataFrame(doc['elements'])\n",
    "\n",
    "    # Subset to most recent (latest dated)\n",
    "    df = df[df['date'] == doc['doc_balancesheetdate']]\n",
    "\n",
    "    total_assets = 0.0\n",
    "    unit = \"NA\"\n",
    "\n",
    "    # Find the total assets by summing components\n",
    "    for each in variable_names:\n",
    "\n",
    "        # Fault-tolerant, will skip whatever isn't numeric\n",
    "        try:\n",
    "            total_assets = total_assets + df[df['name'] == each].iloc[0]['value']\n",
    "\n",
    "            # Retrieve reporting unit if exists\n",
    "            unit = df[df['name'] == each].iloc[0]['unit']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return ({\"total_assets\": total_assets, \"unit\": unit})\n",
    "\n",
    "\n",
    "def summarise_by_priority(doc, variable_names):\n",
    "    \"\"\"\n",
    "    Takes a document (dict) after extraction, and tries to extract\n",
    "    a summary variable relating to the financial state of the enterprise\n",
    "    by looking for each named, in order.  Returns dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    doc -- an extracted document dict, with \"elements\" entry as created\n",
    "           by the 'scrape_clean_elements' functions.\n",
    "    variable_names - variables to find and check if they exist.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert elements to pandas df\n",
    "    df = pd.DataFrame(doc['elements'])\n",
    "\n",
    "    # Subset to most recent (latest dated)\n",
    "    df = df[df['date'] == doc['doc_balancesheetdate']]\n",
    "\n",
    "    primary_assets = 0.0\n",
    "    unit = \"NA\"\n",
    "\n",
    "    # Find the net asset/liability variable by hunting names in order\n",
    "    for each in variable_names:\n",
    "        try:\n",
    "\n",
    "            # Fault tolerant, will skip whatever isn't numeric\n",
    "            primary_assets = df[df['name'] == each].iloc[0]['value']\n",
    "\n",
    "            # Retrieve reporting unit if it exists\n",
    "            unit = df[df['name'] == each].iloc[0]['unit']\n",
    "            break\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return ({\"primary_assets\": primary_assets, \"unit\": unit})\n",
    "\n",
    "\n",
    "def summarise_set(doc, variable_names):\n",
    "    \"\"\"\n",
    "    Takes a document (dict) after extraction, and tries to extract\n",
    "    summary variables relating to the financial state of the enterprise\n",
    "    by returning all those named that exist.  Returns dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    doc -- an extracted document dict, with \"elements\" entry as created\n",
    "           by the 'scrape_clean_elements' functions.\n",
    "    variable_names - variables to find and return if they exist.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Convert elements to pandas df\n",
    "    df = pd.DataFrame(doc['elements'])\n",
    "\n",
    "    # Subset to most recent (latest dated)\n",
    "    df = df[df['date'] == doc['doc_balancesheetdate']]\n",
    "\n",
    "    # Find all the variables of interest should they exist\n",
    "    for each in variable_names:\n",
    "        try:\n",
    "            results[each] = df[df['name'] == each].iloc[0]['value']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Send the variables back to be appended\n",
    "    return (results)\n",
    "\n",
    "\n",
    "def scrape_elements(soup, filepath):\n",
    "    \"\"\"\n",
    "    Parses an XBRL (xml) company accounts file\n",
    "    for all labelled content and extracts the\n",
    "    content (and metadata, eg; unitref) of each\n",
    "    element found to a dictionary \n",
    "\n",
    "    params: filepath (str)\n",
    "    output: list of dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Try multiple methods of retrieving data, I think only the first is\n",
    "    # now needed though.  The rest will be removed after testing this\n",
    "    # but should not affect execution speed.\n",
    "    try:\n",
    "        element_set = soup.find_all()\n",
    "        elements = parse_elements(element_set, soup)\n",
    "        if len(elements) <= 5:\n",
    "            raise Exception(\"Elements should be gte 5, was {}\".format(len(elements)))\n",
    "        return (elements)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (0)\n",
    "\n",
    "\n",
    "def flatten_data(doc):\n",
    "    \"\"\"\n",
    "    Takes the data returned by process account, with its tree-like\n",
    "    structure and reorganises it into a long-thin format table structure\n",
    "    suitable for SQL applications.\n",
    "    \"\"\"\n",
    "\n",
    "    # Need to drop components later, so need copy in function\n",
    "    doc2 = doc.copy()\n",
    "    doc_df = pd.DataFrame()\n",
    "\n",
    "    # Pandas should create series, then columns, from dicts when called\n",
    "    # like this\n",
    "    for element in doc2['elements']:\n",
    "        doc_df = doc_df.append(element, ignore_index=True)\n",
    "\n",
    "    # Dump the \"elements\" entry in the doc dict\n",
    "    doc2.pop(\"elements\")\n",
    "\n",
    "    # Create uniform columns for all other properties\n",
    "    for key in doc2:\n",
    "        doc_df[key] = doc2[key]\n",
    "\n",
    "    return (doc_df)\n",
    "\n",
    "\n",
    "def process_account(filepath):\n",
    "    \"\"\"\n",
    "    Scrape all of the relevant information from\n",
    "    an iXBRL (html) file, upload the elements\n",
    "    and some metadata to a mongodb.\n",
    "\n",
    "    Named arguments:\n",
    "    filepath -- complete filepath (string) from drive root\n",
    "    \"\"\"\n",
    "    doc = {}\n",
    "\n",
    "    # Some metadata, doc name, upload date/time, archive file it came from\n",
    "    doc['doc_name'] = filepath.split(\"/\")[-1]\n",
    "    doc['doc_type'] = filepath.split(\".\")[-1].lower()\n",
    "    doc['doc_upload_date'] = str(datetime.now())\n",
    "    doc['arc_name'] = filepath.split(\"/\")[-2]\n",
    "    doc['parsed'] = True\n",
    "\n",
    "    # Complicated ones\n",
    "    sheet_date = filepath.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    doc['doc_balancesheetdate'] = datetime.strptime(sheet_date, \"%Y%m%d\").date().isoformat()\n",
    "\n",
    "    doc['doc_companieshouseregisterednumber'] = filepath.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-2]\n",
    "\n",
    "#     print(filepath)\n",
    "\n",
    "    try:\n",
    "        soup = BS(open(filepath, \"rb\"), \"html.parser\")\n",
    "    except:\n",
    "        print(\"Failed to open: \" + filepath)\n",
    "        return (1)\n",
    "\n",
    "    # Get metadata about the accounting standard used\n",
    "    try:\n",
    "        doc['doc_standard_type'], doc['doc_standard_date'], doc['doc_standard_link'] = retrieve_accounting_standard(\n",
    "            soup)\n",
    "    except:\n",
    "        doc['doc_standard_type'], doc['doc_standard_date'], doc['doc_standard_link'] = (0, 0, 0)\n",
    "\n",
    "    # Fetch all the marked elements of the document\n",
    "    try:\n",
    "        doc['elements'] = scrape_elements(soup, filepath)\n",
    "    except Exception as e:\n",
    "        doc['parsed'] = False\n",
    "        doc['Error'] = e\n",
    "\n",
    "    try:\n",
    "        return (doc)\n",
    "    except Exception as e:\n",
    "        return (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "<a id='section1'></a>\n",
    "<div style=\"\n",
    "    width: 100%;\n",
    "    height: 100px;\n",
    "    border:none;\n",
    "    background:linear-gradient(-45deg, #d84227, #ce3587);\n",
    "    text-align: center;\n",
    "    position: relative;\n",
    "\">\n",
    "  <span style=\"\n",
    "      font-size: 40px;\n",
    "      padding: 0 10px;\n",
    "      color: white;\n",
    "      margin: 0;\n",
    "      position: absolute;\n",
    "      top: 50%;\n",
    "      left: 50%;\n",
    "      -ms-transform: translate(-50%, -50%);\n",
    "      transform: translate(-50%, -50%);\n",
    "      font-weight: bold;\n",
    "  \">\n",
    "    EXTRACTION <!--Padding is optional-->\n",
    "  </span>\n",
    "</div><a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def get_filepaths(directory):\n",
    "\n",
    "    \"\"\" Helper function - \n",
    "    Get all of the filenames in a directory that\n",
    "    end in htm* or xml.\n",
    "    Under the assumption that all files within\n",
    "    the folder are financial records. \"\"\"\n",
    "\n",
    "    files = [directory + \"/\" + filename\n",
    "                for filename in os.listdir(directory)\n",
    "                    if ((\"htm\" in filename.lower()) or (\"xml\" in filename.lower()))]\n",
    "    return(files)\n",
    "\n",
    "def progressBar(name, value, endvalue, bar_length = 50, width = 20):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent*bar_length) - 1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        sys.stdout.write(\n",
    "            \"\\r{0: <{1}} : [{2}]{3}%   ({4} / {5})\".format(\n",
    "                name,\n",
    "                width,\n",
    "                arrow + spaces,\n",
    "                int(round(percent*100)),\n",
    "                value,\n",
    "                endvalue\n",
    "            )\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "        if value == endvalue:     \n",
    "             sys.stdout.write('\\n\\n')\n",
    "                \n",
    "def retrieve_list_of_tags(dataframe, column, output_folder):\n",
    "    \"\"\"\n",
    "    Save dataframe containing all unique tags to txt format in specified directory.\n",
    "    \n",
    "    Arguements:\n",
    "        dataframe:     tabular data\n",
    "        column:        location of xbrl tags\n",
    "        output_folder: user specified file location\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    year = str(pd.DatetimeIndex(dataframe['doc_balancesheetdate']).year[0])\n",
    "    month = \"{:02d}\".format(pd.DatetimeIndex(dataframe['doc_balancesheetdate']).month[0])\n",
    "    \n",
    "    list_of_tags = results['name'].tolist()\n",
    "    list_of_tags_unique = list(set(list_of_tags))\n",
    "\n",
    "    print(\n",
    "        \"Number of tags in total: {}\\nOf which are unique: {}\".format(len(list_of_tags), len(list_of_tags_unique))\n",
    "    )\n",
    "    \n",
    "    with open(output_folder + \"/\" + year + \"-\" + month + \"_list_of_tags.txt\", 'w') as f:\n",
    "        for item in list_of_tags_unique:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "def get_tag_counts(dataframe, column, output_folder):\n",
    "    \"\"\"\n",
    "    Save dataframe containing all unique tags to txt format in specified directory.\n",
    "    \n",
    "    Arguements:\n",
    "        dataframe:     tabular data\n",
    "        column:        location of xbrl tags\n",
    "        output_folder: user specified file location\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    year = str(pd.DatetimeIndex(dataframe['doc_balancesheetdate']).year[0])\n",
    "    month = \"{:02d}\".format(pd.DatetimeIndex(dataframe['doc_balancesheetdate']).month[0])\n",
    "    \n",
    "    dataframe[\"count\"] = dataframe.groupby(by = column)[column].transform(\"count\")\n",
    "    dataframe.sort_values(\"count\", inplace = True, ascending = False)\n",
    "    dataframe.drop_duplicates(subset = [column, \"count\"], keep = \"first\", inplace = True)\n",
    "    dataframe = dataframe[[column, \"count\"]]\n",
    "    \n",
    "    print(dataframe.shape)\n",
    "    \n",
    "    dataframe.to_csv(\n",
    "        output_folder + \"/\" + year + \"-\" + month + \"_unique_tag_frequencies.csv\",\n",
    "        header = None,\n",
    "        index = True,\n",
    "        sep = \"\\t\",\n",
    "        mode = \"a\"\n",
    "    )\n",
    "    \n",
    "def build_month_table(list_of_files):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    process_start = time.time()\n",
    "    \n",
    "    # Empty table awaiting results\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    COUNT = 0\n",
    "\n",
    "    # For every file\n",
    "    for file in files:\n",
    "\n",
    "        COUNT += 1\n",
    "\n",
    "        # Read the file\n",
    "        doc = process_account(file)\n",
    "\n",
    "        # tabulate the results\n",
    "        doc_df = flatten_data(doc)\n",
    "\n",
    "        # append to table\n",
    "        results = results.append(doc_df)\n",
    "\n",
    "        progressBar(\"XBRL Accounts Parsed\", COUNT, len(files), bar_length = 50, width = 20)\n",
    "    \n",
    "    print(\"Average time to process an XBRL file: \\x1b[31m{:0f}\\x1b[0m\".format((time.time() - process_start) / 60, 2), \"seconds\")\n",
    "    \n",
    "    return results\n",
    "                \n",
    "def output_xbrl_month(dataframe, output_folder, file_type = \"csv\"):\n",
    "    \"\"\"\n",
    "    Save dataframe to csv format in specified directory, with particular naming scheme \"YYYY-MM_xbrl_data.csv\".\n",
    "\n",
    "    Arguments:\n",
    "        dataframe:     tabular data\n",
    "        output_folder: user specified file destination\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    year = str(pd.DatetimeIndex(dataframe['doc_balancesheetdate']).year[0])\n",
    "    month = \"{:02d}\".format(pd.DatetimeIndex(dataframe['doc_balancesheetdate']).month[0])\n",
    "    \n",
    "    if file_type == \"csv\":\n",
    "        dataframe.to_csv(\n",
    "            output_folder\n",
    "                + \"\\\\\"\n",
    "                + str(year)\n",
    "                + \"-\"\n",
    "                + str(month)\n",
    "                + \"_xbrl_data.csv\",\n",
    "            index = False,\n",
    "            header = True\n",
    "        )\n",
    "    else:\n",
    "        print(\"I need a CSV for now...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n"
     ]
    }
   ],
   "source": [
    "# Get all the filenames from the example folder\n",
    "files = get_filepaths(\"D:\\Project_3 - Image_Processing\\example_data_XBRL_iXBRL\")\n",
    "\n",
    "print(len(files))\n",
    "\n",
    "# Here you can splice/truncate the number of files you want to process for testing\n",
    "files = files[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use these commands to inspect portions of the XML data:\n",
    "```python\n",
    "doc = process_account(files[0])\n",
    "\n",
    "# display for fun\n",
    "doc\n",
    "\n",
    "doc['elements']\n",
    "\n",
    "# Loop through the document, retrieving any element with a matching name\n",
    "for element in doc['elements']:\n",
    "    if element['name'] == 'balancesheetdate':\n",
    "        print(element)\n",
    "        \n",
    "# Extract the all the data to long-thin table format for use with SQL\n",
    "# Note, tables from docs should be appendable to one another to create\n",
    "# tables of all data\n",
    "flatten_data(doc).head(15)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBRL Accounts Parsed : [------------------------------------------------->]100%   (30 / 30)\n",
      "\n",
      "Average time to process an XBRL file: \u001b[31m0.245658\u001b[0m seconds\n",
      "(1747, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arc_name</th>\n",
       "      <th>date</th>\n",
       "      <th>doc_balancesheetdate</th>\n",
       "      <th>doc_companieshouseregisterednumber</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_standard_date</th>\n",
       "      <th>doc_standard_link</th>\n",
       "      <th>doc_standard_type</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_upload_date</th>\n",
       "      <th>name</th>\n",
       "      <th>parsed</th>\n",
       "      <th>sign</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>ukcompanieshouseregisterednumber</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>09102728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>entitycurrentlegalorregisteredname</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>SQUARE ONE PROPERTY CONSULTANTS LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>balancesheetdate</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>2017-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>namedirectorsigningaccounts</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>namedirectorsigningaccounts</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>entitydormant</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>startdateforperiodcoveredbyreport</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>enddateforperiodcoveredbyreport</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>2017-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>entitytrading</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D:\\Project_3 - Image_Processing\\example_data_X...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>09102728</td>\n",
       "      <td>Prod223_2125_09102728_20170630.html</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...</td>\n",
       "      <td>uk-gaap-full</td>\n",
       "      <td>html</td>\n",
       "      <td>2020-06-22 15:50:50.225091</td>\n",
       "      <td>calledupsharecapitalnotpaidnotexpressedascurre...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iso4217:GBP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            arc_name        date  \\\n",
       "0  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "1  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "2  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "3  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "4  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "5  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "6  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "7  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "8  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "9  D:\\Project_3 - Image_Processing\\example_data_X...  2017-06-30   \n",
       "\n",
       "  doc_balancesheetdate doc_companieshouseregisterednumber  \\\n",
       "0           2017-06-30                           09102728   \n",
       "1           2017-06-30                           09102728   \n",
       "2           2017-06-30                           09102728   \n",
       "3           2017-06-30                           09102728   \n",
       "4           2017-06-30                           09102728   \n",
       "5           2017-06-30                           09102728   \n",
       "6           2017-06-30                           09102728   \n",
       "7           2017-06-30                           09102728   \n",
       "8           2017-06-30                           09102728   \n",
       "9           2017-06-30                           09102728   \n",
       "\n",
       "                              doc_name doc_standard_date  \\\n",
       "0  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "1  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "2  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "3  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "4  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "5  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "6  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "7  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "8  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "9  Prod223_2125_09102728_20170630.html        2009-09-01   \n",
       "\n",
       "                                   doc_standard_link doc_standard_type  \\\n",
       "0  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "1  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "2  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "3  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "4  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "5  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "6  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "7  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "8  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "9  http://www.xbrl.org/uk/gaap/core/2009-09-01/uk...      uk-gaap-full   \n",
       "\n",
       "  doc_type             doc_upload_date  \\\n",
       "0     html  2020-06-22 15:50:50.225091   \n",
       "1     html  2020-06-22 15:50:50.225091   \n",
       "2     html  2020-06-22 15:50:50.225091   \n",
       "3     html  2020-06-22 15:50:50.225091   \n",
       "4     html  2020-06-22 15:50:50.225091   \n",
       "5     html  2020-06-22 15:50:50.225091   \n",
       "6     html  2020-06-22 15:50:50.225091   \n",
       "7     html  2020-06-22 15:50:50.225091   \n",
       "8     html  2020-06-22 15:50:50.225091   \n",
       "9     html  2020-06-22 15:50:50.225091   \n",
       "\n",
       "                                                name  parsed sign  \\\n",
       "0                   ukcompanieshouseregisterednumber    True  NaN   \n",
       "1                 entitycurrentlegalorregisteredname    True  NaN   \n",
       "2                                   balancesheetdate    True  NaN   \n",
       "3                        namedirectorsigningaccounts    True  NaN   \n",
       "4                        namedirectorsigningaccounts    True  NaN   \n",
       "5                                      entitydormant    True  NaN   \n",
       "6                  startdateforperiodcoveredbyreport    True  NaN   \n",
       "7                    enddateforperiodcoveredbyreport    True  NaN   \n",
       "8                                      entitytrading    True  NaN   \n",
       "9  calledupsharecapitalnotpaidnotexpressedascurre...    True  NaN   \n",
       "\n",
       "          unit                                    value  \n",
       "0           NA                                 09102728  \n",
       "1           NA  SQUARE ONE PROPERTY CONSULTANTS LIMITED  \n",
       "2           NA                               2017-06-30  \n",
       "3           NA                                           \n",
       "4           NA                                           \n",
       "5           NA                                    false  \n",
       "6           NA                               2016-07-01  \n",
       "7           NA                               2017-06-30  \n",
       "8           NA                                     true  \n",
       "9  iso4217:GBP                                        0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, build a table of all variables from all example (digital) documents\n",
    "# This can take a while\n",
    "\n",
    "results = build_month_table(files)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tag:  116\n"
     ]
    }
   ],
   "source": [
    "# Find list of all unqiue tags in dataset\n",
    "\n",
    "list_of_tags = results[\"name\"].tolist()\n",
    "list_of_tags_unique = list(set(list_of_tags))\n",
    "\n",
    "print(\"Longest tag: \", len(max(list_of_tags_unique, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags in total: 1747\n",
      "Of which are unique: 175\n"
     ]
    }
   ],
   "source": [
    "# Output all unqiue tags to a txt file\n",
    "\n",
    "retrieve_list_of_tags(\n",
    "    results,\n",
    "    \"name\",\n",
    "    \"D:\\Project_3 - Image_Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 2)\n"
     ]
    }
   ],
   "source": [
    "# Output all unqiue tags and their relative frequencies to a txt file\n",
    "\n",
    "get_tag_counts(\n",
    "    results,\n",
    "    \"name\",\n",
    "    \"D:\\Project_3 - Image_Processing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi Rob,\n",
    "# \n",
    "# Just checked several example batch files against the parser's outputs and QA checks,\n",
    "# the shape in the above cell is always exactly 1 less than the number of rows in the\n",
    "# outputted csv - as expected with the header row.\n",
    "# \n",
    "# Don't seem to get any error, and I've updated the methods in this notebook further.\n",
    "# \n",
    "# Let me know how you get on with this version!\n",
    "# \n",
    "# Elliott.\n",
    "\n",
    "output_xbrl_month(results, \"D:\\Project_3 - Image_Processing\", file_type = \"csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
