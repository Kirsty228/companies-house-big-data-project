{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<div style=\"\n",
    "    width: 100%;\n",
    "    height: 100px;\n",
    "    border:none;\n",
    "    background:linear-gradient(-45deg, #d84227, #ce3587);\n",
    "    text-align: center;\n",
    "    position: relative;\n",
    "\">\n",
    "  <span style=\"\n",
    "      font-size: 40px;\n",
    "      padding: 0 10px;\n",
    "      color: white;\n",
    "      margin: 0;\n",
    "      position: absolute;\n",
    "      top: 50%;\n",
    "      left: 50%;\n",
    "      -ms-transform: translate(-50%, -50%);\n",
    "      transform: translate(-50%, -50%);\n",
    "      font-weight: bold;\n",
    "  \">\n",
    "    XBRL PARSER <!--Padding is optional-->\n",
    "  </span>\n",
    "</div><a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Martin Wood - Office for National Statistics\n",
    "martin.wood@ons.gov.uk\n",
    "23/07/2018\n",
    "XBRL parser\n",
    "Contains functions that scrape and clean an XBRL document's content\n",
    "and variables, returning a dict ready for dumping into MongoDB.\n",
    "\n",
    "ELliott PHillips - Office for National Statistics\n",
    "eelliott.phillips@ons.gov.uk\n",
    "30/06/2020\n",
    "Code adapted and furthered for Companies House XBRL accounts parsing and outputting.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from bs4 import BeautifulSoup as BS  # Can parse xml or html docs\n",
    "\n",
    "# Table of variables and values that indicate consolidated status\n",
    "consolidation_var_table = {\n",
    "    \"includedinconsolidationsubsidiary\": True,\n",
    "    \"investmententityrequiredtoapplyexceptionfromconsolidationtruefalse\": True,\n",
    "    \"subsidiaryunconsolidatedtruefalse\": False,\n",
    "    \"descriptionreasonwhyentityhasnotpreparedconsolidatedfinancialstatements\": \"exist\",\n",
    "    \"consolidationpolicy\": \"exist\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_value(string):\n",
    "    \"\"\"\n",
    "    Take a value that's stored as a string,\n",
    "    clean it and convert to numeric.\n",
    "\n",
    "    If it's just a dash, it's taken to mean\n",
    "    zero.\n",
    "    \"\"\"\n",
    "    if string.strip() == \"-\":\n",
    "        return (0.0)\n",
    "\n",
    "    try:\n",
    "        return float(string.strip().replace(\",\", \"\").replace(\" \", \"\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (string)\n",
    "\n",
    "\n",
    "def retrieve_from_context(soup, contextref):\n",
    "    \"\"\"\n",
    "    Used where an element of the document contained no data, only a\n",
    "    reference to a context element.\n",
    "    Finds the relevant context element and retrieves the relevant data.\n",
    "\n",
    "    Returns a text string\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    contextref -- the id of the context element to be raided\n",
    "    \"\"\"\n",
    "    try:\n",
    "        context = soup.find(\"xbrli:context\", id=contextref)\n",
    "        contents = context.find(\"xbrldi:explicitmember\").get_text().split(\":\")[-1].strip()\n",
    "\n",
    "    except:\n",
    "        contents = \"\"\n",
    "\n",
    "    return (contents)\n",
    "\n",
    "\n",
    "def retrieve_accounting_standard(soup):\n",
    "    \"\"\"\n",
    "    Gets the account reporting standard in use in a document by hunting\n",
    "    down the link to the schema reference sheet that always appears to\n",
    "    be in the document, and extracting the format and standard date from\n",
    "    the string of the url itself.\n",
    "    WARNING - That means that there's a lot of implicity hardcoded info\n",
    "    on the way these links are formated and referenced, within this\n",
    "    function.  Might need changing someday.\n",
    "\n",
    "    Returns a 3-tuple (standard, date, original url)\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the relevant link by its unique attribute\n",
    "    link_obj = soup.find(\"link:schemaref\")\n",
    "\n",
    "    # If we didn't find anything it's an xml doc using a different\n",
    "    # element name:\n",
    "    if link_obj == None:\n",
    "        link_obj = soup.find(\"schemaref\")\n",
    "\n",
    "    # extract the name of the .xsd schema file, which contains format\n",
    "    # and date information\n",
    "    text = link_obj['xlink:href'].split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Split the extracted text into format and date, return values\n",
    "    return (text[:-10].strip(\"-\"), text[-10:], link_obj['xlink:href'])\n",
    "\n",
    "\n",
    "def retrieve_unit(soup, each):\n",
    "    \"\"\"\n",
    "    Gets the reporting unit by trying to chase a unitref to\n",
    "    its source, alternatively uses element attribute unitref\n",
    "    if it's not a reference to another element.\n",
    "\n",
    "    Returns the unit\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    each -- element of BeautifulSoup souped object\n",
    "    \"\"\"\n",
    "\n",
    "    # If not, try to discover the unit string in the\n",
    "    # soup object\n",
    "    try:\n",
    "        unit_str = soup.find(id=each['unitref']).get_text()\n",
    "\n",
    "    except:\n",
    "        # Or if not, in the attributes of the element\n",
    "        try:\n",
    "            unit_str = each.attrs['unitref']\n",
    "\n",
    "        except:\n",
    "            return (\"NA\")\n",
    "\n",
    "    return (unit_str.strip())\n",
    "\n",
    "\n",
    "def retrieve_date(soup, each):\n",
    "    \"\"\"\n",
    "    Gets the reporting date by trying to chase a contextref\n",
    "    to its source and extract its period, alternatively uses\n",
    "    element attribute contextref if it's not a reference\n",
    "    to another element.\n",
    "    Returns the date\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup souped html/xml object\n",
    "    each -- element of BeautifulSoup souped object\n",
    "    \"\"\"\n",
    "\n",
    "    # Try to find a date tag within the contextref element,\n",
    "    # starting with the most specific tags, and starting with\n",
    "    # those for ixbrl docs as it's the most common file.\n",
    "    date_tag_list = [\"xbrli:enddate\",\n",
    "                     \"xbrli:instant\",\n",
    "                     \"xbrli:period\",\n",
    "                     \"enddate\",\n",
    "                     \"instant\",\n",
    "                     \"period\"]\n",
    "\n",
    "    for tag in date_tag_list:\n",
    "        try:\n",
    "            date_str = each['contextref']\n",
    "            date_val = parser.parse(soup.find(id=each['contextref']).find(tag).get_text()). \\\n",
    "                date(). \\\n",
    "                isoformat()\n",
    "            return (date_val)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        date_str = each.attrs['contextref']\n",
    "        date_val = parser.parse(each.attrs['contextref']). \\\n",
    "            date(). \\\n",
    "            isoformat()\n",
    "        return (date_val)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (\"NA\")\n",
    "\n",
    "\n",
    "def parse_element(soup, element):\n",
    "    \"\"\"\n",
    "    For a discovered XBRL tagged element, go through, retrieve its name\n",
    "    and value and associated metadata.\n",
    "\n",
    "    Keyword arguments:\n",
    "    soup -- BeautifulSoup object of accounts document\n",
    "    element -- soup object of discovered tagged element\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"contextref\" not in element.attrs:\n",
    "        return ({})\n",
    "\n",
    "    element_dict = {}\n",
    "\n",
    "    # Basic name and value\n",
    "    try:\n",
    "        # Method for XBRLi docs first\n",
    "        element_dict['name'] = element.attrs['name'].lower().split(\":\")[-1]\n",
    "    except:\n",
    "        # Method for XBRL docs second\n",
    "        element_dict['name'] = element.name.lower().split(\":\")[-1]\n",
    "\n",
    "    element_dict['value'] = element.get_text()\n",
    "    element_dict['unit'] = retrieve_unit(soup, element)\n",
    "    element_dict['date'] = retrieve_date(soup, element)\n",
    "\n",
    "    # If there's no value retrieved, try raiding the associated context data\n",
    "    if element_dict['value'] == \"\":\n",
    "        element_dict['value'] = retrieve_from_context(soup, element.attrs['contextref'])\n",
    "\n",
    "    # If the value has a defined unit (eg a currency) convert to numeric\t\n",
    "    if element_dict['unit'] != \"NA\":\n",
    "        element_dict['value'] = clean_value(element_dict['value'])\n",
    "\n",
    "    # Retrieve sign of element if exists\n",
    "    try:\n",
    "        element_dict['sign'] = element.attrs['sign']\n",
    "\n",
    "        # if it's negative, convert the value then and there\n",
    "        if element_dict['sign'].strip() == \"-\":\n",
    "            element_dict['value'] = 0.0 - element_dict['value']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (element_dict)\n",
    "\n",
    "\n",
    "def parse_elements(element_set, soup):\n",
    "    \"\"\"\n",
    "    For a set of discovered elements within a document, try to parse\n",
    "    them.  Only keep valid results (test is whether field \"name\"\n",
    "    exists).\n",
    "\n",
    "    Keyword arguments:\n",
    "    element_set -- BeautifulSoup iterable search result object\n",
    "    soup -- BeautifulSoup object of accounts document\n",
    "    \"\"\"\n",
    "    elements = []\n",
    "    for each in element_set:\n",
    "        element_dict = parse_element(soup, each)\n",
    "        if 'name' in element_dict:\n",
    "            elements.append(element_dict)\n",
    "    return (elements)\n",
    "\n",
    "\n",
    "def summarise_by_sum(doc, variable_names):\n",
    "    \"\"\"\n",
    "    Takes a document (dict) after extraction, and tries to extract\n",
    "    a summary variable relating to the financial state of the enterprise\n",
    "    by summing all those named that exist.  Returns dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    doc -- an extracted document dict, with \"elements\" entry as created\n",
    "           by the 'scrape_clean_elements' functions.\n",
    "    variable_names - variables to find and sum if they exist\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert elements to pandas df\n",
    "    df = pd.DataFrame(doc['elements'])\n",
    "\n",
    "    # Subset to most recent (latest dated)\n",
    "    df = df[df['date'] == doc['doc_balancesheetdate']]\n",
    "\n",
    "    total_assets = 0.0\n",
    "    unit = \"NA\"\n",
    "\n",
    "    # Find the total assets by summing components\n",
    "    for each in variable_names:\n",
    "\n",
    "        # Fault-tolerant, will skip whatever isn't numeric\n",
    "        try:\n",
    "            total_assets = total_assets + df[df['name'] == each].iloc[0]['value']\n",
    "\n",
    "            # Retrieve reporting unit if exists\n",
    "            unit = df[df['name'] == each].iloc[0]['unit']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return ({\"total_assets\": total_assets, \"unit\": unit})\n",
    "\n",
    "\n",
    "def summarise_by_priority(doc, variable_names):\n",
    "    \"\"\"\n",
    "    Takes a document (dict) after extraction, and tries to extract\n",
    "    a summary variable relating to the financial state of the enterprise\n",
    "    by looking for each named, in order.  Returns dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    doc -- an extracted document dict, with \"elements\" entry as created\n",
    "           by the 'scrape_clean_elements' functions.\n",
    "    variable_names - variables to find and check if they exist.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert elements to pandas df\n",
    "    df = pd.DataFrame(doc['elements'])\n",
    "\n",
    "    # Subset to most recent (latest dated)\n",
    "    df = df[df['date'] == doc['doc_balancesheetdate']]\n",
    "\n",
    "    primary_assets = 0.0\n",
    "    unit = \"NA\"\n",
    "\n",
    "    # Find the net asset/liability variable by hunting names in order\n",
    "    for each in variable_names:\n",
    "        try:\n",
    "\n",
    "            # Fault tolerant, will skip whatever isn't numeric\n",
    "            primary_assets = df[df['name'] == each].iloc[0]['value']\n",
    "\n",
    "            # Retrieve reporting unit if it exists\n",
    "            unit = df[df['name'] == each].iloc[0]['unit']\n",
    "            break\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return ({\"primary_assets\": primary_assets, \"unit\": unit})\n",
    "\n",
    "\n",
    "def summarise_set(doc, variable_names):\n",
    "    \"\"\"\n",
    "    Takes a document (dict) after extraction, and tries to extract\n",
    "    summary variables relating to the financial state of the enterprise\n",
    "    by returning all those named that exist.  Returns dict.\n",
    "\n",
    "    Keyword arguments:\n",
    "    doc -- an extracted document dict, with \"elements\" entry as created\n",
    "           by the 'scrape_clean_elements' functions.\n",
    "    variable_names - variables to find and return if they exist.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Convert elements to pandas df\n",
    "    df = pd.DataFrame(doc['elements'])\n",
    "\n",
    "    # Subset to most recent (latest dated)\n",
    "    df = df[df['date'] == doc['doc_balancesheetdate']]\n",
    "\n",
    "    # Find all the variables of interest should they exist\n",
    "    for each in variable_names:\n",
    "        try:\n",
    "            results[each] = df[df['name'] == each].iloc[0]['value']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Send the variables back to be appended\n",
    "    return (results)\n",
    "\n",
    "\n",
    "def scrape_elements(soup, filepath):\n",
    "    \"\"\"\n",
    "    Parses an XBRL (xml) company accounts file\n",
    "    for all labelled content and extracts the\n",
    "    content (and metadata, eg; unitref) of each\n",
    "    element found to a dictionary \n",
    "\n",
    "    params: filepath (str)\n",
    "    output: list of dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Try multiple methods of retrieving data, I think only the first is\n",
    "    # now needed though.  The rest will be removed after testing this\n",
    "    # but should not affect execution speed.\n",
    "    try:\n",
    "        element_set = soup.find_all()\n",
    "        elements = parse_elements(element_set, soup)\n",
    "        if len(elements) <= 5:\n",
    "            raise Exception(\"Elements should be gte 5, was {}\".format(len(elements)))\n",
    "        \n",
    "    except:\n",
    "        #if fails parsing create dummy entry elements so entry still exists in dictonary\n",
    "        elements = [{'name': None, 'value': None, 'unit': None, 'date': None}]\n",
    "        pass\n",
    "\n",
    "    return (elements)\n",
    "\n",
    "\n",
    "def flatten_data(doc):\n",
    "    \"\"\"\n",
    "    Takes the data returned by process account, with its tree-like\n",
    "    structure and reorganises it into a long-thin format table structure\n",
    "    suitable for SQL applications.\n",
    "    \"\"\"\n",
    "\n",
    "    # Need to drop components later, so need copy in function\n",
    "    doc2 = doc.copy()\n",
    "    doc_df = pd.DataFrame()\n",
    "\n",
    "    # Pandas should create series, then columns, from dicts when called\n",
    "    # like this\n",
    "    \n",
    "    if not isinstance(doc2['elements'], int):\n",
    "        for element in doc2['elements']:\n",
    "            doc_df = doc_df.append(element, ignore_index=True)\n",
    "\n",
    "    # Dump the \"elements\" entry in the doc dict\n",
    "    doc2.pop(\"elements\")\n",
    "\n",
    "    # Create uniform columns for all other properties\n",
    "    for key in doc2:\n",
    "        doc_df[key] = doc2[key]\n",
    "    return (doc_df)\n",
    "\n",
    "def amp_correction(filepath):\n",
    "    \"\"\"\n",
    "    Looks for & symbol in html file and uses regex to strip out any instances that are not strictly amp;\n",
    "    \n",
    "    Named arguments:\n",
    "    filepath -- complete filepath (string) from drive root\n",
    "    \"\"\"\n",
    "    #import re \n",
    "    \n",
    "    #open file as txt\n",
    "    #file = open(filepath, 'r')\n",
    "\n",
    "    #corrected_file = \"\"\n",
    "    #for line in file:\n",
    "        #line = line.strip()\n",
    "        #print(line)\n",
    "        \n",
    "    #return file \n",
    "        #too generalised!\n",
    "        #determine if any lines need correcting\n",
    "        #if re.search(r'&(?!amp;)',line) is not None:\n",
    "            #print(line)\n",
    "            #print message to show correction\n",
    "            #print('file {} is being corrected for a misplaced & sign, may cause some strange parsing'.format(file))\n",
    "            #replace with acceptable syntax\n",
    "            #line = re.sub(r'&(?!amp;)',r'&amp;',line)\n",
    "    #append subbed lines back to file\n",
    "    #corrected_file += line\n",
    "    #return corrected file\n",
    "    \n",
    "    #return(corrected_file)\n",
    "\n",
    "\n",
    "def process_account(filepath):\n",
    "    \"\"\"\n",
    "    Scrape all of the relevant information from\n",
    "    an iXBRL (html) file, upload the elements\n",
    "    and some metadata to a mongodb.\n",
    "\n",
    "    Named arguments:\n",
    "    filepath -- complete filepath (string) from drive root\n",
    "    \"\"\"\n",
    "    doc = {}\n",
    "\n",
    "    # Some metadata, doc name, upload date/time, archive file it came from\n",
    "    doc['doc_name'] = filepath.split(\"/\")[-1]\n",
    "    doc['doc_type'] = filepath.split(\".\")[-1].lower()\n",
    "    doc['doc_upload_date'] = str(datetime.now())\n",
    "    doc['arc_name'] = filepath.split(\"/\")[-2]\n",
    "    doc['parsed'] = True\n",
    "\n",
    "    # Complicated ones\n",
    "    sheet_date = filepath.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    doc['doc_balancesheetdate'] = datetime.strptime(sheet_date, \"%Y%m%d\").date().isoformat()\n",
    "\n",
    "    doc['doc_companieshouseregisterednumber'] = filepath.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-2]\n",
    "\n",
    "    try:\n",
    "        soup = BS(open(filepath, \"rb\"), \"html.parser\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Failed to open: \" + filepath)\n",
    "        return (1)\n",
    "\n",
    "    # Get metadata about the accounting standard used\n",
    "    try:\n",
    "        doc['doc_standard_type'], doc['doc_standard_date'], doc['doc_standard_link'] = retrieve_accounting_standard(soup)\n",
    "        doc['parsed'] = True\n",
    "    except:\n",
    "        doc['doc_standard_type'], doc['doc_standard_date'], doc['doc_standard_link'] = (0, 0, 0)\n",
    "        doc['parsed'] = False\n",
    "\n",
    "    # Fetch all the marked elements of the document\n",
    "    try:\n",
    "        doc['elements'] = scrape_elements(soup, filepath)\n",
    "    except Exception as e:\n",
    "        doc['parsed'] = False\n",
    "        doc['Error'] = e\n",
    "\n",
    "    try:\n",
    "        return (doc)\n",
    "    except Exception as e:\n",
    "        return (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "<a id='section1'></a>\n",
    "<div style=\"\n",
    "    width: 100%;\n",
    "    height: 100px;\n",
    "    border:none;\n",
    "    background:linear-gradient(-45deg, #d84227, #ce3587);\n",
    "    text-align: center;\n",
    "    position: relative;\n",
    "\">\n",
    "  <span style=\"\n",
    "      font-size: 40px;\n",
    "      padding: 0 10px;\n",
    "      color: white;\n",
    "      margin: 0;\n",
    "      position: absolute;\n",
    "      top: 50%;\n",
    "      left: 50%;\n",
    "      -ms-transform: translate(-50%, -50%);\n",
    "      transform: translate(-50%, -50%);\n",
    "      font-weight: bold;\n",
    "  \">\n",
    "    EXTRACTION <!--Padding is optional-->\n",
    "  </span>\n",
    "</div><a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def get_filepaths(directory):\n",
    "\n",
    "    \"\"\" Helper function - \n",
    "    Get all of the filenames in a directory that\n",
    "    end in htm* or xml.\n",
    "    Under the assumption that all files within\n",
    "    the folder are financial records. \"\"\"\n",
    "\n",
    "    files = [directory + \"/\" + filename\n",
    "                for filename in os.listdir(directory)\n",
    "                    if ((\"htm\" in filename.lower()) or (\"xml\" in filename.lower()))]\n",
    "    \n",
    "    month_and_year = ('').join(directory.split('-')[-1:])\n",
    "    month, year = month_and_year[:-4], month_and_year[-4:]\n",
    "    \n",
    "    return files, month, year\n",
    "\n",
    "def progressBar(name, value, endvalue, bar_length = 50, width = 20):\n",
    "        percent = float(value) / endvalue\n",
    "        arrow = '-' * int(round(percent*bar_length) - 1) + '>'\n",
    "        spaces = ' ' * (bar_length - len(arrow))\n",
    "        sys.stdout.write(\n",
    "            \"\\r{0: <{1}} : [{2}]{3}%   ({4} / {5})\".format(\n",
    "                name,\n",
    "                width,\n",
    "                arrow + spaces,\n",
    "                int(round(percent*100)),\n",
    "                value,\n",
    "                endvalue\n",
    "            )\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "        if value == endvalue:     \n",
    "             sys.stdout.write('\\n\\n')\n",
    "                \n",
    "def retrieve_list_of_tags(dataframe, column, output_folder):\n",
    "    \"\"\"\n",
    "    Save dataframe containing all unique tags to txt format in specified directory.\n",
    "    \n",
    "    Arguements:\n",
    "        dataframe:     tabular data\n",
    "        column:        location of xbrl tags\n",
    "        output_folder: user specified file location\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    list_of_tags = results['name'].tolist()\n",
    "    list_of_tags_unique = list(set(list_of_tags))\n",
    "\n",
    "    print(\n",
    "        \"Number of tags in total: {}\\nOf which are unique: {}\".format(len(list_of_tags), len(list_of_tags_unique))\n",
    "    )\n",
    "    \n",
    "    with open(output_folder + \"/\" + folder_year + \"-\" + folder_month + \"_list_of_tags.txt\", 'w') as f:\n",
    "        for item in list_of_tags_unique:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "def get_tag_counts(dataframe, column, output_folder):\n",
    "    \"\"\"\n",
    "    Save dataframe containing all unique tags to txt format in specified directory.\n",
    "    \n",
    "    Arguements:\n",
    "        dataframe:     tabular data\n",
    "        column:        location of xbrl tags\n",
    "        output_folder: user specified file location\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cache = dataframe\n",
    "    cache[\"count\"] = cache.groupby(by = column)[column].transform(\"count\")\n",
    "    cache.sort_values(\"count\", inplace = True, ascending = False)\n",
    "    cache.drop_duplicates(subset = [column, \"count\"], keep = \"first\", inplace = True)\n",
    "    cache = cache[[column, \"count\"]]\n",
    "    \n",
    "    print(cache.shape)\n",
    "    \n",
    "    cache.to_csv(\n",
    "        output_folder + \"/\" + folder_year + \"-\" + folder_month + \"_unique_tag_frequencies.csv\",\n",
    "        header = None,\n",
    "        index = True,\n",
    "        sep = \"\\t\",\n",
    "        mode = \"a\"\n",
    "    )\n",
    "    \n",
    "def build_month_table(list_of_files):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    process_start = time.time()\n",
    "    \n",
    "    # Empty table awaiting results\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    COUNT = 0\n",
    "\n",
    "    # For every file\n",
    "    #for file in files:\n",
    "    for file in files:\n",
    "        COUNT += 1\n",
    "        \n",
    "        # Read the file\n",
    "        doc = process_account(file)\n",
    "\n",
    "        # tabulate the results\n",
    "        doc_df = flatten_data(doc)\n",
    "\n",
    "        # append to table\n",
    "        results = results.append(doc_df)\n",
    "\n",
    "        progressBar(\"XBRL Accounts Parsed\", COUNT, len(files), bar_length = 50, width = 20)\n",
    "    \n",
    "    print(\"Average time to process an XBRL file: \\x1b[31m{:0f}\\x1b[0m\".format((time.time() - process_start) / 60, 2), \"seconds\")\n",
    "    \n",
    "    return results\n",
    "                \n",
    "def output_xbrl_month(dataframe, output_folder, file_type = \"csv\"):\n",
    "    \"\"\"\n",
    "    Save dataframe to csv format in specified directory, with particular naming scheme \"YYYY-MM_xbrl_data.csv\".\n",
    "\n",
    "    Arguments:\n",
    "        dataframe:     tabular data\n",
    "        output_folder: user specified file destination\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if file_type == \"csv\":\n",
    "        dataframe.to_csv(\n",
    "            output_folder\n",
    "                + \"/\"\n",
    "                + folder_year\n",
    "                + \"-\"\n",
    "                + folder_month\n",
    "                + \"_xbrl_data.csv\",\n",
    "            index = False,\n",
    "            header = True\n",
    "        )\n",
    "    else:\n",
    "        print(\"I need a CSV for now...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61697\n"
     ]
    }
   ],
   "source": [
    "# Get all the filenames from the example folder\n",
    "files, folder_month, folder_year = get_filepaths(\"/shares/data/20200519_companies_house_accounts/xbrl_unpacked_data/Accounts_monthly_Data-June2011\")\n",
    "\n",
    "print(len(files))\n",
    "\n",
    "# Here you can splice/truncate the number of files you want to process for testing\n",
    "#files = files[791:793]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "June 2011\n"
     ]
    }
   ],
   "source": [
    "print(folder_month, folder_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use these commands to inspect portions of the XML data:\n",
    "```python\n",
    "doc = process_account(files[0:])\n",
    "\n",
    "# display for fun\n",
    "doc\n",
    "\n",
    "doc['elements']\n",
    "\n",
    "# Loop through the document, retrieving any element with a matching name\n",
    "for element in doc['elements']:\n",
    "    if element['name'] == 'balancesheetdate':\n",
    "        print(element)\n",
    "        \n",
    "# Extract the all the data to long-thin table format for use with SQL\n",
    "# Note, tables from docs should be appendable to one another to create\n",
    "# tables of all data\n",
    "flatten_data(doc).head(15)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBRL Accounts Parsed : [------------------------------------------------->]100%   (2 / 2)\n",
      "\n",
      "Average time to process an XBRL file: \u001b[31m0.012294\u001b[0m seconds\n",
      "(70, 14)\n",
      "2 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_upload_date</th>\n",
       "      <th>arc_name</th>\n",
       "      <th>parsed</th>\n",
       "      <th>doc_balancesheetdate</th>\n",
       "      <th>doc_companieshouseregisterednumber</th>\n",
       "      <th>doc_standard_type</th>\n",
       "      <th>doc_standard_date</th>\n",
       "      <th>doc_standard_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Prod224_9967_01082116_20101231.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.371453</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>01082116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>companynotdormant</td>\n",
       "      <td>NA</td>\n",
       "      <td>true</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>companieshouseregisterednumber</td>\n",
       "      <td>NA</td>\n",
       "      <td>01082277</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>entitycurrentlegalname</td>\n",
       "      <td>NA</td>\n",
       "      <td>MANYHILL LIMITED</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>balancesheetdate</td>\n",
       "      <td>NA</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>tangiblefixedassets</td>\n",
       "      <td>iso4217:GBP</td>\n",
       "      <td>17078</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>tangiblefixedassets</td>\n",
       "      <td>iso4217:GBP</td>\n",
       "      <td>14311</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>tangiblefixedassetscostorvaluation</td>\n",
       "      <td>iso4217:GBP</td>\n",
       "      <td>82421</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>tangiblefixedassetscostorvaluation</td>\n",
       "      <td>iso4217:GBP</td>\n",
       "      <td>85721</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>tangiblefixedassetsadditions</td>\n",
       "      <td>iso4217:GBP</td>\n",
       "      <td>10500</td>\n",
       "      <td>Prod224_9967_01082277_20100930.xml</td>\n",
       "      <td>xml</td>\n",
       "      <td>2020-08-13 15:05:33.413685</td>\n",
       "      <td>Accounts_monthly_Data-June2011</td>\n",
       "      <td>True</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>01082277</td>\n",
       "      <td>uk-gaap-ae</td>\n",
       "      <td>2008-04-06</td>\n",
       "      <td>http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                name         unit  \\\n",
       "0        None                                None         None   \n",
       "0  2010-09-30                   companynotdormant           NA   \n",
       "1  2010-09-30      companieshouseregisterednumber           NA   \n",
       "2  2010-09-30              entitycurrentlegalname           NA   \n",
       "3  2010-09-30                    balancesheetdate           NA   \n",
       "4  2010-09-30                 tangiblefixedassets  iso4217:GBP   \n",
       "5  2009-09-30                 tangiblefixedassets  iso4217:GBP   \n",
       "6  2010-09-30  tangiblefixedassetscostorvaluation  iso4217:GBP   \n",
       "7  2009-09-30  tangiblefixedassetscostorvaluation  iso4217:GBP   \n",
       "8  2010-09-30        tangiblefixedassetsadditions  iso4217:GBP   \n",
       "\n",
       "              value                            doc_name doc_type  \\\n",
       "0              None  Prod224_9967_01082116_20101231.xml      xml   \n",
       "0              true  Prod224_9967_01082277_20100930.xml      xml   \n",
       "1          01082277  Prod224_9967_01082277_20100930.xml      xml   \n",
       "2  MANYHILL LIMITED  Prod224_9967_01082277_20100930.xml      xml   \n",
       "3        2010-09-30  Prod224_9967_01082277_20100930.xml      xml   \n",
       "4             17078  Prod224_9967_01082277_20100930.xml      xml   \n",
       "5             14311  Prod224_9967_01082277_20100930.xml      xml   \n",
       "6             82421  Prod224_9967_01082277_20100930.xml      xml   \n",
       "7             85721  Prod224_9967_01082277_20100930.xml      xml   \n",
       "8             10500  Prod224_9967_01082277_20100930.xml      xml   \n",
       "\n",
       "              doc_upload_date                        arc_name  parsed  \\\n",
       "0  2020-08-13 15:05:33.371453  Accounts_monthly_Data-June2011   False   \n",
       "0  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "1  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "2  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "3  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "4  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "5  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "6  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "7  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "8  2020-08-13 15:05:33.413685  Accounts_monthly_Data-June2011    True   \n",
       "\n",
       "  doc_balancesheetdate doc_companieshouseregisterednumber doc_standard_type  \\\n",
       "0           2010-12-31                           01082116                 0   \n",
       "0           2010-09-30                           01082277        uk-gaap-ae   \n",
       "1           2010-09-30                           01082277        uk-gaap-ae   \n",
       "2           2010-09-30                           01082277        uk-gaap-ae   \n",
       "3           2010-09-30                           01082277        uk-gaap-ae   \n",
       "4           2010-09-30                           01082277        uk-gaap-ae   \n",
       "5           2010-09-30                           01082277        uk-gaap-ae   \n",
       "6           2010-09-30                           01082277        uk-gaap-ae   \n",
       "7           2010-09-30                           01082277        uk-gaap-ae   \n",
       "8           2010-09-30                           01082277        uk-gaap-ae   \n",
       "\n",
       "  doc_standard_date                                  doc_standard_link  \n",
       "0                 0                                                  0  \n",
       "0        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "1        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "2        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "3        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "4        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "5        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "6        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "7        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  \n",
       "8        2008-04-06  http://www.companieshouse.gov.uk/ef/xbrl/uk/fr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, build a table of all variables from all example (digital) documents\n",
    "# This can take a while\n",
    "    \n",
    "results = build_month_table(files)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "print(results.doc_name.drop_duplicates().count(),len(files))\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi Rob,\n",
    "# \n",
    "# Just checked several example batch files against the parser's outputs and QA checks,\n",
    "# the shape in the above cell is always exactly 1 less than the number of rows in the\n",
    "# outputted csv - as expected with the header row.\n",
    "# \n",
    "# Don't seem to get any error, and I've updated the methods in this notebook further.\n",
    "# \n",
    "# Let me know how you get on with this version!\n",
    "# \n",
    "# Elliott.\n",
    "\n",
    "output_xbrl_month(results, \"/shares/data/20200519_companies_house_accounts/xbrl_parsed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-50d41aee1bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist_of_tags_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Longest tag: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tags_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Find list of all unqiue tags in dataset\n",
    "\n",
    "list_of_tags = results[\"name\"].tolist()\n",
    "list_of_tags_unique = list(set(list_of_tags))\n",
    "\n",
    "print(\"Longest tag: \", len(max(list_of_tags_unique, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all unqiue tags to a txt file\n",
    "\n",
    "retrieve_list_of_tags(\n",
    "    results,\n",
    "    \"name\",\n",
    "    \"/shares/data/20200519_companies_house_accounts/logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all unqiue tags and their relative frequencies to a txt file\n",
    "\n",
    "get_tag_counts(\n",
    "    results,\n",
    "    \"name\",\n",
    "    \"/shares/data/20200519_companies_house_accounts/logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
